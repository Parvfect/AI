{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioBERT_setup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhyzlkiqZOog8eGaTPtQBV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parvfect/AI/blob/master/BioBERT_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DsQpvpcJcmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a6c88e-ae60-4c8d-e760-4e8526dd82c6"
      },
      "source": [
        "# Mounting Google Drive and importing biobert files from the repository that was uploaded to the drive\n",
        "# Alternatively, you could clone the repository\n",
        "from google.colab import drive\n",
        "drive.mount('/biobert/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /biobert/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l1oxkhAtYg0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3X5TtwdKmRI"
      },
      "source": [
        "Setup for Bio BERT imported from - https://github.com/dmis-lab/biobert \n",
        "\n",
        "Bio BERT is a biomedical text mining transformer model that has been typically trained for three objectives, that is, \n",
        "\n",
        "1) Named Entity Recognition\n",
        "2) Relational Extraction\n",
        "3) Question Answering\n",
        "\n",
        "In this notebook, I am going to go through setting up the modules to run, train and evaluate its performance as directed in the paper - https://academic.oup.com/bioinformatics/article/36/4/1234/5566506"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkn7BP5uNWd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b89ada7-7174-4614-cc07-f687fa31ea7f"
      },
      "source": [
        "# Downloading the pre-processed datasets to fine train the model for evaulation\n",
        "# Note that I have imported the dataset from my drive\n",
        "#!./download.sh\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "%cd biobert/\n",
        "%cd MyDrive/\n",
        "%cd biobert/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/biobert/MyDrive\n",
            "/biobert\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "/\n",
            "\u001b[0m\u001b[01;34mbin\u001b[0m/      \u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mlib\u001b[0m/    \u001b[01;34mmnt\u001b[0m/         \u001b[01;34mroot\u001b[0m/  \u001b[01;34msys\u001b[0m/                \u001b[01;34musr\u001b[0m/\n",
            "\u001b[01;34mbiobert\u001b[0m/  \u001b[01;34mdev\u001b[0m/      \u001b[01;34mlib32\u001b[0m/  \u001b[01;34mopt\u001b[0m/         \u001b[01;34mrun\u001b[0m/   \u001b[01;34mtensorflow-1.15.2\u001b[0m/  \u001b[01;34mvar\u001b[0m/\n",
            "\u001b[01;34mboot\u001b[0m/     \u001b[01;34metc\u001b[0m/      \u001b[01;34mlib64\u001b[0m/  \u001b[01;34mproc\u001b[0m/        \u001b[01;34msbin\u001b[0m/  \u001b[30;42mtmp\u001b[0m/\n",
            "\u001b[01;34mcontent\u001b[0m/  \u001b[01;34mhome\u001b[0m/     \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mpython-apt\u001b[0m/  \u001b[01;34msrv\u001b[0m/   \u001b[01;34mtools\u001b[0m/\n",
            "/biobert\n",
            "\u001b[0m\u001b[01;34mMyDrive\u001b[0m/\n",
            "/biobert/MyDrive\n",
            " 11.gdoc\n",
            " 20210416_080403000_iOS.png\n",
            "'210208 Dream Directory - Club Page Content Collection Form.gsheet'\n",
            "'AI Manual - Chimera.gdoc'\n",
            "'A Modern Approach to Verbal  Non-Verbal Reasoning by R.S. Aggarwal (z-lib.org).pdf'\n",
            "'April 8 Meeting.gdoc'\n",
            "'Artist concerts \\.gdoc'\n",
            " BCI.gdoc\n",
            " \u001b[0m\u001b[01;34mbiobert\u001b[0m/\n",
            "'Block Theory: Offical Rulebook.gdoc'\n",
            " BT00151100046123255_RLS.pdf\n",
            " Calendar.gdoc\n",
            "'chem thingty.gdoc'\n",
            "'CISCE Results 2018.pdf'\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "'COmp projectp.gdoc'\n",
            "'Copy of Event Registration.gform'\n",
            "'Copy of GH012544.MP4'\n",
            "'Courses (1).gdoc'\n",
            " Courses.gdoc\n",
            "'=cricket.gdoc'\n",
            " \u001b[01;34mCS\u001b[0m/\n",
            "'CS practicalfile.gdoc'\n",
            " dataFile\n",
            " \u001b[01;34mdecoded\u001b[0m/\n",
            "'Disarmament of foreign military in war torn nations.gdoc'\n",
            "'Dream Directory - Club Page Content Collection Form.gsheet'\n",
            " Durham.gdoc\n",
            "\u001b[01;34m'Ecorale'\\''s game of tag (File responses)'\u001b[0m/\n",
            "'English Project.gdoc'\n",
            "\u001b[01;34m'Event Registration (File responses)'\u001b[0m/\n",
            "'Event Registration.gform'\n",
            " explorere.gdoc\n",
            "'Fake news.gdoc'\n",
            "'Five month plan.gdoc'\n",
            "'geo project.gdoc'\n",
            "'Getting started.pdf'\n",
            " GH012544.MP4\n",
            " GH012547.MP4\n",
            " GSL.gdoc\n",
            " GX010029-5.m4v\n",
            " GX010030-6.m4v\n",
            " GX010031-7.m4v\n",
            " GX010157.MP4\n",
            "'Hi im parv.gdoc'\n",
            " H.O.S.T..gdoc\n",
            "'In its grasp.gdoc'\n",
            "'JUSTICE 1.0.gdoc'\n",
            "'Lab notes.gdoc'\n",
            "'Lady of Roses.docx'\n",
            " Literature.gdoc\n",
            " LSBG.gdoc\n",
            "'May 5 Meeting.gdoc'\n",
            "'Mobility workout(daily preferably).gdoc'\n",
            "'Monthly budget.gsheet'\n",
            " November.gdoc\n",
            "'October first week.gdoc'\n",
            " passport.jpeg\n",
            "'Personal Statement lol not final yet.gdoc'\n",
            " Phil.gdoc\n",
            "'Physics 12th Project(Semiconductors).gdoc'\n",
            "'Podar Summit.docx'\n",
            "'Podar Summit.gdoc'\n",
            " Poems.gdoc\n",
            "'Practical file.gdoc'\n",
            "'Record (online-voice-recorder.com).mp3'\n",
            "'References for self teaching rocketry.gdoc'\n",
            "'Relation between mean and median.gdoc'\n",
            "'Resume (1).gdoc'\n",
            " Resume.gdoc\n",
            "'Schedule 4.0.gdoc'\n",
            "'Schedule DLW.gdoc'\n",
            "'Schedule f2.0.gdoc'\n",
            "'September last week.gdoc'\n",
            " \u001b[01;34mShared\u001b[0m/\n",
            "\u001b[01;34m'Shared (1)'\u001b[0m/\n",
            " Tempest.gdoc\n",
            "'The book.gdoc'\n",
            "'The Eleventh hour operation.gdoc'\n",
            "'The Gift of India.gdoc'\n",
            "'The Magical Spell.gdoc'\n",
            "'Things to take.gsheet'\n",
            " thingy.gdoc\n",
            "'Thomas Frank'\\''s Budget Modeler Template!.gsheet'\n",
            "'Three day rule.gdoc'\n",
            "'Untitled document (10).gdoc'\n",
            "'Untitled document (11).gdoc'\n",
            "'Untitled document (12).gdoc'\n",
            "'Untitled document (13).gdoc'\n",
            "'Untitled document (14).gdoc'\n",
            "'Untitled document (15).gdoc'\n",
            "'Untitled document (16).gdoc'\n",
            "'Untitled document (17).gdoc'\n",
            "'Untitled document (18).gdoc'\n",
            "'Untitled document (19).gdoc'\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document (20).gdoc'\n",
            "'Untitled document (21).gdoc'\n",
            "'Untitled document (22).gdoc'\n",
            "'Untitled document (23).gdoc'\n",
            "'Untitled document (24).gdoc'\n",
            "'Untitled document (25).gdoc'\n",
            "'Untitled document (26).gdoc'\n",
            "'Untitled document (27).gdoc'\n",
            "'Untitled document (28).gdoc'\n",
            "'Untitled document (29).gdoc'\n",
            "'Untitled document (2).gdoc'\n",
            "'Untitled document (30).gdoc'\n",
            "'Untitled document (31).gdoc'\n",
            "'Untitled document (32).gdoc'\n",
            "'Untitled document (33).gdoc'\n",
            "'Untitled document (34).gdoc'\n",
            "'Untitled document (35).gdoc'\n",
            "'Untitled document (36).gdoc'\n",
            "'Untitled document (37).gdoc'\n",
            "'Untitled document (38).gdoc'\n",
            "'Untitled document (39).gdoc'\n",
            "'Untitled document (3).gdoc'\n",
            "'Untitled document (40).gdoc'\n",
            "'Untitled document (41).gdoc'\n",
            "'Untitled document (42).gdoc'\n",
            "'Untitled document (43).gdoc'\n",
            "'Untitled document (44).gdoc'\n",
            "'Untitled document (45).gdoc'\n",
            "'Untitled document (46).gdoc'\n",
            "'Untitled document (47).gdoc'\n",
            "'Untitled document (4).gdoc'\n",
            "'Untitled document (5).gdoc'\n",
            "'Untitled document (6).gdoc'\n",
            "'Untitled document (7).gdoc'\n",
            "'Untitled document (8).gdoc'\n",
            "'Untitled document (9).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled spreadsheet (1).gsheet'\n",
            "'Untitled spreadsheet.gsheet'\n",
            "'Week 1 (1).docx'\n",
            "'Week 1.docx'\n",
            "'Week 1.gdoc'\n",
            "'What now.gdoc'\n",
            "'WhatsApp Image 2021-04-26 at 10.22.57 PM.jpeg'\n",
            " Workout.gdoc\n",
            " WP.gdoc\n",
            "/biobert/MyDrive/biobert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MsBE6ShJvV3",
        "outputId": "e79b4451-9f25-4cbd-f6fd-c4e8c09ee4ab"
      },
      "source": [
        "!git clone https://github.com/dmis-lab/biobert.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'biobert'...\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 332 (delta 30), reused 8 (delta 3), pack-reused 278\u001b[K\n",
            "Receiving objects: 100% (332/332), 508.52 KiB | 2.57 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqf-0kI1LeVw",
        "outputId": "d5ce7231-4821-4ccc-9dd1-f366c869e7f0"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
        "\n",
        "# Importing the latest release of the BioBERT model\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting pandas==0.23\n",
            "  Downloading pandas-0.23.0.tar.gz (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 17.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.40.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pandas==0.23->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas==0.23->-r requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (4.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 6)) (1.0.1)\n",
            "Building wheels for collected packages: pandas, gast\n",
            "  Building wheel for pandas (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pandas\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pandas\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=3869a977c27f79175febdc00173e58f91eb4b1d785e769b16e73b90312957d82\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Failed to build pandas\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu, pandas\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "    Running setup.py install for pandas ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of pandas\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/pandas-1.1.5.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~andas-1.1.5.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/pandas/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~andas\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-bpo0nnma/pandas_2cd71e2102974b279460f83744fd63db/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-bpo0nnma/pandas_2cd71e2102974b279460f83744fd63db/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-n_3lu9o1/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/pandas Check the logs for full command output.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE23q8rRMDUt"
      },
      "source": [
        "# Setting environment variables for training parameters\n",
        "import os\n",
        "\n",
        "# Model directory environment variable\n",
        "os.environ['BIOBERT_DIR'] = \"./biobert/MyDrive/biobert/biobert_v1.1_pubmed\"\n",
        "\n",
        "# Named Entity Recognition Environment Variables\n",
        "os.environ['NER_DIR'] = \"./biobert/MyDrive/biobert/datasets/NER/NCBI-disease\"  \n",
        "os.environ['OUTPUT_DIR'] = \"./biobert/MyDrive/biobert/ner_outputs\"  \n",
        "\n",
        "# Relational Extraction Environment Variables\n",
        "os.environ['RE_DIR'] = \"./biobert/MyDrive/biobert/datasets/RE/GAD/1\"  \n",
        "os.environ['TASK_NAME'] = \"/biobert/MyDrive/biobertgad\"  \n",
        "os.environ['OUTPUT_DIR'] = \"./biobert/MyDrive/biobert/re_outputs_1\" \n",
        "\n",
        "# Question Answering Environment Variables\n",
        "os.environ['QA_DIR'] = \"./biobert/MyDrive/biobert/datasets/QA/BioASQ\" \n",
        "os.environ['OUTPUT_DIR'] = \"./biobert/MyDrive/biobert/qa_outputs\" \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smdQXAWnPJff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e43e1da-2e07-47a9-fc5d-fba1a437a43c"
      },
      "source": [
        "# Running Named Entity Recognition\n",
        "%mkdir -p $OUTPUT_DIR\n",
        "\n",
        "!python run_ner.py --do_train=true --do_eval=true --vocab_file=$BIOBERT_DIR/vocab.txt --bert_config_file=$BIOBERT_DIR/bert_config.json --init_checkpoint=$BIOBERT_DIR/model.ckpt-1000000 --num_train_epochs=10.0 --data_dir=$NER_DIR --output_dir=$OUTPUT_DIR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /biobert/My Drive/biobert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/absl/flags/_validators.py:356: UserWarning: Flag --task_name has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n",
            "  'command line!' % flag_name)\n",
            "WARNING:tensorflow:From run_ner.py:651: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From run_ner.py:466: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0924 15:08:37.541520 140026833954688 module_wrapper.py:139] From run_ner.py:466: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From run_ner.py:466: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0924 15:08:37.541781 140026833954688 module_wrapper.py:139] From run_ner.py:466: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /biobert/My Drive/biobert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0924 15:08:37.542110 140026833954688 module_wrapper.py:139] From /biobert/My Drive/biobert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_ner.py\", line 651, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"run_ner.py\", line 473, in main\n",
            "    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
            "  File \"/biobert/My Drive/biobert/modeling.py\", line 93, in from_json_file\n",
            "    text = reader.read()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 122, in read\n",
            "    self._preread_check()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 84, in _preread_check\n",
            "    compat.as_bytes(self.__name), 1024 * 512)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: ./biobert/MyDrive/biobert/biobert_v1.1_pubmed/bert_config.json; No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ7lArfkPc24"
      },
      "source": [
        "You can change the arguments as you want. Once you have trained your model, you can use it in inference mode by using --do_train=false --do_predict=true for evaluating test.tsv. The token-level evaluation result will be printed as stdout format. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPHIFtknPqua"
      },
      "source": [
        "Note that this result is the token-level evaluation measure while the official evaluation should use the entity-level evaluation measure. The results of python run_ner.py will be recorded as two files: token_test.txt and label_test.txt in $OUTPUT_DIR. Use ./biocodes/ner_detokenize.py to obtain word level prediction file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atbah8BfPum_"
      },
      "source": [
        "!python biocodes/ner_detokenize.py --token_test_path=$OUTPUT_DIR/token_test.txt --label_test_path=$OUTPUT_DIR/label_test.txt --answer_path=$NER_DIR/test.tsv --output_dir=$OUTPUT_DIR\n",
        "\n",
        "# This will generate NER_result_conll.txt in $OUTPUT_DIR. Use ./biocodes/conlleval.pl for entity-level exact match evaluation results.\n",
        "\n",
        "!perl biocodes/conlleval.pl < $OUTPUT_DIR/NER_result_conll.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rf2uuKWP7d-"
      },
      "source": [
        "Note that this is a sample run of an NER model. The performance of NER models usually converges at more than 50 epochs (learning rate = 1e-5 is recommended)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtEVYamQQG8x"
      },
      "source": [
        "# Running Relational Extraction\n",
        "! python run_re.py --task_name=$TASK_NAME --do_train=true --do_eval=true --do_predict=true --vocab_file=$BIOBERT_DIR/vocab.txt --bert_config_file=$BIOBERT_DIR/bert_config.json --init_checkpoint=$BIOBERT_DIR/model.ckpt-1000000 --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --do_lower_case=false --data_dir=$RE_DIR --output_dir=$OUTPUT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftmbLDd0QQrX"
      },
      "source": [
        "The predictions will be saved into a file called test_results.tsv in the $OUTPUT_DIR. Use ./biocodes/re_eval.py for the evaluation. Note that the CHEMPROT dataset is a multi-class classification dataset and to evaluate the CHEMPROT result, you should run re_eval.py with additional --task=chemprot flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0oaYvVxQQAf"
      },
      "source": [
        "!python ./biocodes/re_eval.py --output_path=$OUTPUT_DIR/test_results.tsv --answer_path=$RE_DIR/test.tsv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf80dKXYQadO"
      },
      "source": [
        "Please be aware that you have to change $OUTPUT_DIR to train/test a new model. For instance, as most RE datasets are in 10-fold, you have to make a different output directory to train/test a model for a different fold (e.g.,  !export OUTPUT_DIR=./re_outputs_2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J703vTQfQo1E"
      },
      "source": [
        "# Running Question Answering\n",
        "!python run_qa.py --do_train=True --do_predict=True --vocab_file=$BIOBERT_DIR/vocab.txt --bert_config_file=$BIOBERT_DIR/bert_config.json --init_checkpoint=$BIOBERT_DIR/model.ckpt-1000000 --max_seq_length=384 --train_batch_size=12 --learning_rate=5e-6 --doc_stride=128 --num_train_epochs=5.0 --do_lower_case=False --train_file=$QA_DIR/BioASQ-train-factoid-4b.json --predict_file=$QA_DIR/BioASQ-test-factoid-4b-1.json --output_dir=$OUTPUT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w3dILLpRJgj"
      },
      "source": [
        "The predictions will be saved into a file called predictions.json and nbest_predictions.json in $OUTPUT_DIR. Run ./biocodes/transform_nbset2bioasqform.py to convert nbest_predictions.json to the BioASQ JSON format, which will be used for the official evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqu-0BAbRF3t"
      },
      "source": [
        "!python ./biocodes/transform_nbset2bioasqform.py --nbest_path=$OUTPUT_DIR/nbest_predictions.json --output_path=$OUTPUT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpsPlVphRSsz"
      },
      "source": [
        "This will generate BioASQform_BioASQ-answer.json in $OUTPUT_DIR. Clone evaluation code from BioASQ github and run evaluation code on Evaluation-Measures directory. Please note that you should always put 5 as parameter for -e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OctInXc2RNzV"
      },
      "source": [
        "!git clone https://github.com/BioASQ/Evaluation-Measures.git\n",
        "!cd Evaluation-Measures\n",
        "java -Xmx10G -cp $CLASSPATH:./flat/BioASQEvaluation/dist/BioASQEvaluation.jar evaluation.EvaluatorTask1b -phaseB -e 5 ../$QA_DIR/4B1_golden.json ../$OUTPUT_DIR/BioASQform_BioASQ-answer.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ0XMPozRekc"
      },
      "source": [
        "The second, third and fourth numbers will be SAcc, LAcc, and MRR of factoid questions respectively"
      ]
    }
  ]
}